{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "What? Improve performance with ensembles\n",
    "\n",
    "The three most popular methods for combining the predictions from different models are:\n",
    "[1] BAGGING: [1.1] bgged decistion tres. [1.2] Random forest [1.3] Extra trees\n",
    "[2] BOOSTING: [2.1] AdaBoost, [2.2] Stochastic gradient boosting\n",
    "[3] VOTING: \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold\n",
    "from IPython.display import Markdown, display\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size:  (768, 8)\n",
      "Labels size:  (768,)\n"
     ]
    }
   ],
   "source": [
    "filename = '../DATASETS/pima-indians-diabetes.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class'] \n",
    "dataframe = read_csv(filename, names = names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "num_folds = 10\n",
    "print(\"Input size: \", X.shape)\n",
    "print(\"Labels size: \", Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BAGGING algorithms: Bagged Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Bagging performs best with algorithms that have HIGH VARIANCE.\n",
    "A popular example are decision trees, often constructed \n",
    "without pruning.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.7578, standard deviation: 0.0386\n"
     ]
    }
   ],
   "source": [
    "seed = 7\n",
    "kfold = KFold(n_splits=10, shuffle = True, random_state = seed)\n",
    "cart = DecisionTreeClassifier()\n",
    "num_trees = 100\n",
    "model = BaggingClassifier(base_estimator=cart, n_estimators=num_trees, random_state=seed)\n",
    "results = cross_val_score(model, X, Y, cv = kfold)\n",
    "print(f\"Mean: {results.mean():.4f}, standard deviation: {results.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BAGGING algorithms: Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Random Forests is an extension of bagged decision trees. \n",
    "Samples of the training dataset are taken with replacement, \n",
    "but the trees are constructed in a way that reduces the \n",
    "correlation between individual classifiers. Specifically, \n",
    "rather than greedily choosing the best split point in the construction of each tree, only a random subset of features are considered for each split\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.7734, standard deviation: 0.0518\n"
     ]
    }
   ],
   "source": [
    "num_trees = 100\n",
    "max_features = 3\n",
    "kfold = KFold(n_splits=10, shuffle = True, random_state=7)\n",
    "model = RandomForestClassifier(n_estimators=num_trees, max_features=max_features) \n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(f\"Mean: {results.mean():.4f}, standard deviation: {results.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BAGGING algorithms: Extra trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extra Trees are another modification of bagging where \n",
    "random trees are constructed from samples of the training dataset\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.7578, standard deviation: 0.0483\n"
     ]
    }
   ],
   "source": [
    "num_trees = 100\n",
    "max_features = 7\n",
    "kfold = KFold(n_splits=10, shuffle = True, random_state=7)\n",
    "model = ExtraTreesClassifier(n_estimators=num_trees, max_features=max_features)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOOSTING algorithms: AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "AdaBoost was perhaps the first successful boosting ensemble algorithm. \n",
    "It generally works by weighting instances in the dataset by how easy \n",
    "or difficult they are to classify, allowing the algorithm to pay or \n",
    "less attention to them in the construction of subsequent models.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.7553, standard deviation: 0.0371\n"
     ]
    }
   ],
   "source": [
    "num_trees = 30\n",
    "seed=7\n",
    "kfold = KFold(n_splits=10, shuffle = True, random_state=seed)\n",
    "model = AdaBoostClassifier(n_estimators=num_trees, random_state=seed)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(f\"Mean: {results.mean():.4f}, standard deviation: {results.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOOSTING algorithms: Stochastic Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "It is also a technique that is proving to be perhaps one of the \n",
    "best techniques available for improving performance via ensembles.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.7605, standard deviation: 0.0537\n"
     ]
    }
   ],
   "source": [
    "seed = 7\n",
    "num_trees = 100\n",
    "kfold = KFold(n_splits=10, shuffle = True, random_state=seed)\n",
    "model = GradientBoostingClassifier(n_estimators=num_trees, random_state=seed) \n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(f\"Mean: {results.mean():.4f}, standard deviation: {results.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Voting is one of the simplest ways of combining the predictions from multiple \n",
    "machine learning algorithms. It works by first creating two or more standalone \n",
    "models from your training dataset. A Voting Classifier can then be used to wrap\n",
    "your models and average the predictions of the sub-models when asked to make predictions for new data.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.7696, standard deviation: 0.0508\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=10, shuffle = True, random_state=7)\n",
    "# create the sub models\n",
    "estimators = []\n",
    "model1 = LogisticRegression(max_iter = 250)\n",
    "estimators.append(('logistic', model1))\n",
    "model2 = DecisionTreeClassifier()\n",
    "estimators.append(('cart', model2))\n",
    "model3 = SVC()\n",
    "estimators.append(('svm', model3))\n",
    "# create the ensemble model\n",
    "ensemble = VotingClassifier(estimators)\n",
    "results = cross_val_score(ensemble, X, Y, cv=kfold)\n",
    "print(f\"Mean: {results.mean():.4f}, standard deviation: {results.std():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "trainingAI",
   "language": "python",
   "name": "trainingai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
